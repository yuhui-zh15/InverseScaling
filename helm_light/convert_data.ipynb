{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open(\"original_data.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = []\n",
    "for item in data[\"request_states\"]:\n",
    "    item = item[\"instance\"]\n",
    "    question = item[\"input\"]\n",
    "    choices = [reference[\"output\"] for reference in item[\"references\"]]\n",
    "    answer = 0 if item[\"references\"][0][\"tags\"] == ['correct'] else 1\n",
    "    data_processed.append({\n",
    "        \"question\": question,\n",
    "        \"choices\": choices,\n",
    "        \"answer\": answer,\n",
    "    })\n",
    "\n",
    "with open(\"processed_data.jsonl\", \"w\") as f:\n",
    "    for item in data_processed:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open(\"original_zeroshot_davinci.jsonl\"))\n",
    "\n",
    "data_light = [json.loads(line) for line in open(\"dumps/results_100instances_prompt_zeroshot_openai_davinci_20230116_015628.jsonl\") if line.startswith(\"{\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2answer = {}\n",
    "question2ref = {}\n",
    "for item in data[\"request_states\"]:\n",
    "    question = item[\"instance\"][\"input\"]\n",
    "    completions = item[\"result\"][\"completions\"]\n",
    "    choice = completions[0][\"text\"]\n",
    "    question2answer[question] = [choice]\n",
    "    question2ref[question] = [item]\n",
    "\n",
    "for item in data_light:\n",
    "    question = item[\"data\"][\"question\"]\n",
    "    choice = item[\"completion\"]\n",
    "    question2answer[question].append(choice)\n",
    "    question2ref[question].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lev Naumov does not play ___ ?\n",
      "[' A', ' B']\n",
      "{'instance': {'input': 'Lev Naumov does not play ___ ?', 'references': [{'output': 'piano', 'tags': []}, {'output': 'harp', 'tags': ['correct']}], 'split': 'test', 'id': 'id522'}, 'train_trial_index': 0, 'output_mapping': {'A': 'piano', 'B': 'harp'}, 'request': {'model': 'openai/davinci', 'embedding': False, 'prompt': 'The following are multiple choice questions (with answers) about common sense.\\n\\nQuestion: Lev Naumov does not play ___ ?\\nA. piano\\nB. harp\\nAnswer:', 'temperature': 0, 'num_completions': 1, 'top_k_per_token': 1, 'max_tokens': 1, 'stop_sequences': [], 'echo_prompt': False, 'top_p': 1, 'presence_penalty': 0, 'frequency_penalty': 0}, 'result': {'success': True, 'embedding': [], 'completions': [{'text': ' A', 'logprob': -1.0466944, 'tokens': [{'text': ' A', 'logprob': -1.0466944, 'top_logprobs': {' A': -1.0466944}}]}], 'cached': True, 'request_time': 1.4886655807495117, 'request_datetime': 1666752761}, 'num_train_instances': 0, 'prompt_truncated': False, 'num_conditioning_tokens': 0}\n",
      "{'prompt': 'The following are multiple choice questions (with answers) about common sense.\\n\\nQuestion: Lev Naumov does not play ___ ?\\nA. piano\\nB. harp\\nAnswer:', 'success': True, 'completion': ' B', 'top_tokens': [[[' B', -1.0361801], [' A', -1.0615022], [' C', -2.3133218], [' Lev', -3.4448447], [' D', -3.7138953], [' (', -3.8260934], [' \"', -4.2031875], ['\\n', -4.6894627], [' ', -4.8982787], [' a', -5.0781684], [' The', -5.1756644], ['\\xa0', -5.414987], ['A', -5.5957117], ['B', -5.679555], [' He', -5.8798075], [' It', -6.2081885], [' This', -6.2652082], [' Na', -6.347961], [' b', -6.3587413], [' None', -6.490331], [\" '\", -6.8890467], [' __', -6.9525404], [' 1', -6.999811], [' Both', -7.0010433], [' Neither', -7.0170145], ['\\n\\n', -7.033036], [' ___', -7.046743], [' There', -7.0655665], [' -', -7.085441], [' 2', -7.096259], [' In', -7.150457], [' Answer', -7.152398], [' Not', -7.170277], ['C', -7.174758], [' [', -7.216809], [' _', -7.2224474], [' Piano', -7.300357], [' If', -7.3790927], [' All', -7.392993], [' No', -7.4979963], [' the', -7.5411296], [' c', -7.568261], [' piano', -7.5716734], ['bytes: \\\\xe2\\\\x80', -7.576643], [' What', -7.5888305], [' As', -7.59604], [' he', -7.647559], [' none', -7.655294], [' We', -7.6658463], [' E', -7.69383]]], 'data': {'question': 'Lev Naumov does not play ___ ?', 'choices': ['piano', 'harp'], 'answer': 1, 'prompt': 'The following are multiple choice questions (with answers) about common sense.\\n\\nQuestion: Lev Naumov does not play ___ ?\\nA. piano\\nB. harp\\nAnswer:'}}\n",
      "----------------------------------------------------------------\n",
      "The native language of Louis Barthou is not ___ ?\n",
      "[' A', ' B']\n",
      "{'instance': {'input': 'The native language of Louis Barthou is not ___ ?', 'references': [{'output': 'french', 'tags': []}, {'output': 'mortar', 'tags': ['correct']}], 'split': 'test', 'id': 'id572'}, 'train_trial_index': 0, 'output_mapping': {'A': 'french', 'B': 'mortar'}, 'request': {'model': 'openai/davinci', 'embedding': False, 'prompt': 'The following are multiple choice questions (with answers) about common sense.\\n\\nQuestion: The native language of Louis Barthou is not ___ ?\\nA. french\\nB. mortar\\nAnswer:', 'temperature': 0, 'num_completions': 1, 'top_k_per_token': 1, 'max_tokens': 1, 'stop_sequences': [], 'echo_prompt': False, 'top_p': 1, 'presence_penalty': 0, 'frequency_penalty': 0}, 'result': {'success': True, 'embedding': [], 'completions': [{'text': ' A', 'logprob': -1.1806197, 'tokens': [{'text': ' A', 'logprob': -1.1806197, 'top_logprobs': {' A': -1.1806197}}]}], 'cached': True, 'request_time': 0.26840710639953613, 'request_datetime': 1666752770}, 'num_train_instances': 0, 'prompt_truncated': False, 'num_conditioning_tokens': 0}\n",
      "{'prompt': 'The following are multiple choice questions (with answers) about common sense.\\n\\nQuestion: The native language of Louis Barthou is not ___ ?\\nA. french\\nB. mortar\\nAnswer:', 'success': True, 'completion': ' B', 'top_tokens': [[[' B', -1.1808214], [' A', -1.1926165], [' C', -2.4504976], [' (', -3.3583994], [' D', -3.3818789], [' \"', -3.9523363], [' The', -3.9543924], [' Louis', -4.2570505], ['\\n', -4.3067484], [' a', -4.4619226], [' ', -4.598542], [' b', -4.96119], ['A', -5.059207], [' It', -5.180455], [' This', -5.331361], ['\\xa0', -5.392779], ['B', -5.573529], [' French', -5.583661], [' None', -5.7396593], [' c', -5.8126917], [' Not', -5.874909], [' In', -6.07224], [' He', -6.115144], [' the', -6.1218348], [' Neither', -6.2395177], [' Common', -6.2743883], [' Correct', -6.4613347], [' Yes', -6.467208], [' Both', -6.4807673], [' ___', -6.5657434], [\" '\", -6.5917935], ['C', -6.620576], [' -', -6.6665673], [' There', -6.668956], ['\\n\\n', -6.6720624], [' No', -6.676457], [' Answer', -6.756078], [' __', -6.807324], [' As', -6.8147078], [' If', -6.8354726], [' 1', -6.8611283], [' french', -6.8725557], [' [', -6.88387], [' We', -6.965867], [' it', -7.0506067], [' What', -7.060562], [' Although', -7.0739446], [' You', -7.083939], [' d', -7.1108785], [' False', -7.1225057]]], 'data': {'question': 'The native language of Louis Barthou is not ___ ?', 'choices': ['french', 'mortar'], 'answer': 1, 'prompt': 'The following are multiple choice questions (with answers) about common sense.\\n\\nQuestion: The native language of Louis Barthou is not ___ ?\\nA. french\\nB. mortar\\nAnswer:'}}\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# show the difference\n",
    "for question, choices in question2answer.items():\n",
    "    if choices[0] != choices[1]:\n",
    "        print(question)\n",
    "        print(choices)\n",
    "        print(question2ref[question][0])\n",
    "        print(question2ref[question][1])\n",
    "        print(\"-\" * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40801cbea77c0a79d9078044a3ed1f5c584e36b3aad3c05dab13f6f27ed4dea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
